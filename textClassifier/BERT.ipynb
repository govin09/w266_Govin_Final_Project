{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('FinaldataNLP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Topic_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Destruction/Consequences</td>\n",
       "      <td>0.2297</td>\n",
       "      <td>global, warming, park, stop, joshua, admin, na...</td>\n",
       "      <td>['global', 'wine', 'production', 'reach', 'new...</td>\n",
       "      <td>climatechange wine plastics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>News/Media</td>\n",
       "      <td>0.2824</td>\n",
       "      <td>threat, news, happening, temperature, day, fac...</td>\n",
       "      <td>['im', 'dismissing', 'crazy', 'conspiracy', 'i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Belief/Sentiment</td>\n",
       "      <td>0.4131</td>\n",
       "      <td>world, trump, head, president, study, way, sup...</td>\n",
       "      <td>['let', 'turn', 'thing', 'around']</td>\n",
       "      <td>ProtectWhatYouLove ActOnClimate Sustainability...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Arctic/Icecap</td>\n",
       "      <td>0.5074</td>\n",
       "      <td>it, time, year, arctic, planet, weather, probl...</td>\n",
       "      <td>['new', 'approach', 'globalwarming', 'projecti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ClimateChangeIsReal/FightClimateChange</td>\n",
       "      <td>0.7851</td>\n",
       "      <td>climate, change, zinke, real, human, talk, wor...</td>\n",
       "      <td>['child', 'estimated', 'bear', 'burden', 'dise...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>328764</td>\n",
       "      <td>Energy/Emission/Carbon/</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>tree, energy, big, fuel, carbon, gas, get, rot...</td>\n",
       "      <td>['please', 'start', 'voting', 'candidate', 'ca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>328768</td>\n",
       "      <td>Energy/Emission/Carbon/</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>tree, energy, big, fuel, carbon, gas, get, rot...</td>\n",
       "      <td>['year', 'projectpresident', 'donald', 'j', 't...</td>\n",
       "      <td>YEARSproject WarOnOurFuture</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>328775</td>\n",
       "      <td>News/Media</td>\n",
       "      <td>0.2705</td>\n",
       "      <td>threat, news, happening, temperature, day, fac...</td>\n",
       "      <td>['pollutionwatch', 'air', 'contamination', 'dr...</td>\n",
       "      <td>ClimateChange</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>328777</td>\n",
       "      <td>xxForeign</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>scientist, security, trump, extreme, u, worse,...</td>\n",
       "      <td>['data', 'compelling', 'look', 'smaller', 'dat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>328790</td>\n",
       "      <td>ClimateChangeIsReal/FightClimateChange</td>\n",
       "      <td>0.3068</td>\n",
       "      <td>climate, change, zinke, real, human, talk, wor...</td>\n",
       "      <td>['debbie', 'climate', 'change', 'badcorrectthe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Document_No                          Dominant_Topic  \\\n",
       "0                0                Destruction/Consequences   \n",
       "1                1                              News/Media   \n",
       "2                2                        Belief/Sentiment   \n",
       "3                3                           Arctic/Icecap   \n",
       "4                4  ClimateChangeIsReal/FightClimateChange   \n",
       "...            ...                                     ...   \n",
       "99995       328764                 Energy/Emission/Carbon/   \n",
       "99996       328768                 Energy/Emission/Carbon/   \n",
       "99997       328775                              News/Media   \n",
       "99998       328777                               xxForeign   \n",
       "99999       328790  ClimateChangeIsReal/FightClimateChange   \n",
       "\n",
       "       Topic_Perc_Contrib                                           Keywords  \\\n",
       "0                  0.2297  global, warming, park, stop, joshua, admin, na...   \n",
       "1                  0.2824  threat, news, happening, temperature, day, fac...   \n",
       "2                  0.4131  world, trump, head, president, study, way, sup...   \n",
       "3                  0.5074  it, time, year, arctic, planet, weather, probl...   \n",
       "4                  0.7851  climate, change, zinke, real, human, talk, wor...   \n",
       "...                   ...                                                ...   \n",
       "99995              0.1880  tree, energy, big, fuel, carbon, gas, get, rot...   \n",
       "99996              0.2140  tree, energy, big, fuel, carbon, gas, get, rot...   \n",
       "99997              0.2705  threat, news, happening, temperature, day, fac...   \n",
       "99998              0.2583  scientist, security, trump, extreme, u, worse,...   \n",
       "99999              0.3068  climate, change, zinke, real, human, talk, wor...   \n",
       "\n",
       "                                                    Text  \\\n",
       "0      ['global', 'wine', 'production', 'reach', 'new...   \n",
       "1      ['im', 'dismissing', 'crazy', 'conspiracy', 'i...   \n",
       "2                     ['let', 'turn', 'thing', 'around']   \n",
       "3      ['new', 'approach', 'globalwarming', 'projecti...   \n",
       "4      ['child', 'estimated', 'bear', 'burden', 'dise...   \n",
       "...                                                  ...   \n",
       "99995  ['please', 'start', 'voting', 'candidate', 'ca...   \n",
       "99996  ['year', 'projectpresident', 'donald', 'j', 't...   \n",
       "99997  ['pollutionwatch', 'air', 'contamination', 'dr...   \n",
       "99998  ['data', 'compelling', 'look', 'smaller', 'dat...   \n",
       "99999  ['debbie', 'climate', 'change', 'badcorrectthe...   \n",
       "\n",
       "                                                hashtags  Topic_no  \n",
       "0                            climatechange wine plastics         1  \n",
       "1                                                    NaN         2  \n",
       "2      ProtectWhatYouLove ActOnClimate Sustainability...         3  \n",
       "3                                                    NaN         4  \n",
       "4                                                    NaN         5  \n",
       "...                                                  ...       ...  \n",
       "99995                                                NaN        11  \n",
       "99996                        YEARSproject WarOnOurFuture        11  \n",
       "99997                                      ClimateChange         2  \n",
       "99998                                                NaN         8  \n",
       "99999                                                NaN         5  \n",
       "\n",
       "[100000 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClimateChangeIsReal/FightClimateChange    25378\n",
       "Action/Efforts/Awareness                  19469\n",
       "Destruction/Consequences                  11917\n",
       "Belief/Sentiment                          10280\n",
       "Arctic/Icecap                              7802\n",
       "Energy/Emission/Carbon/                    7453\n",
       "Politics/Policy/Law                        5027\n",
       "PolarBear/Wildlife                         3742\n",
       "xxForeign                                  3737\n",
       "Paris Agreement                            2735\n",
       "News/Media                                 2460\n",
       "Name: Dominant_Topic, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Dominant_Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Destruction/Consequences': 0,\n",
       " 'News/Media': 1,\n",
       " 'Belief/Sentiment': 2,\n",
       " 'Arctic/Icecap': 3,\n",
       " 'ClimateChangeIsReal/FightClimateChange': 4,\n",
       " 'Politics/Policy/Law': 5,\n",
       " 'Action/Efforts/Awareness': 6,\n",
       " 'xxForeign': 7,\n",
       " 'PolarBear/Wildlife': 8,\n",
       " 'Paris Agreement': 9,\n",
       " 'Energy/Emission/Carbon/': 10}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_labels = df.Dominant_Topic.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.Dominant_Topic.replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Topic_no</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Action/Efforts/Awareness</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">6</th>\n",
       "      <th>train</th>\n",
       "      <td>16549</td>\n",
       "      <td>16549</td>\n",
       "      <td>16549</td>\n",
       "      <td>16549</td>\n",
       "      <td>7559</td>\n",
       "      <td>16549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>2920</td>\n",
       "      <td>2920</td>\n",
       "      <td>2920</td>\n",
       "      <td>2920</td>\n",
       "      <td>1342</td>\n",
       "      <td>2920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Arctic/Icecap</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>train</th>\n",
       "      <td>6632</td>\n",
       "      <td>6632</td>\n",
       "      <td>6632</td>\n",
       "      <td>6632</td>\n",
       "      <td>2834</td>\n",
       "      <td>6632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>1170</td>\n",
       "      <td>1170</td>\n",
       "      <td>1170</td>\n",
       "      <td>1170</td>\n",
       "      <td>491</td>\n",
       "      <td>1170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Belief/Sentiment</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>8738</td>\n",
       "      <td>8738</td>\n",
       "      <td>8738</td>\n",
       "      <td>8738</td>\n",
       "      <td>3791</td>\n",
       "      <td>8738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>1542</td>\n",
       "      <td>1542</td>\n",
       "      <td>1542</td>\n",
       "      <td>1542</td>\n",
       "      <td>635</td>\n",
       "      <td>1542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ClimateChangeIsReal/FightClimateChange</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>train</th>\n",
       "      <td>21571</td>\n",
       "      <td>21571</td>\n",
       "      <td>21571</td>\n",
       "      <td>21571</td>\n",
       "      <td>6166</td>\n",
       "      <td>21571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>3807</td>\n",
       "      <td>3807</td>\n",
       "      <td>3807</td>\n",
       "      <td>3807</td>\n",
       "      <td>1082</td>\n",
       "      <td>3807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Destruction/Consequences</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>10129</td>\n",
       "      <td>10129</td>\n",
       "      <td>10129</td>\n",
       "      <td>10129</td>\n",
       "      <td>2821</td>\n",
       "      <td>10129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>1788</td>\n",
       "      <td>1788</td>\n",
       "      <td>1788</td>\n",
       "      <td>1788</td>\n",
       "      <td>505</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Energy/Emission/Carbon/</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">10</th>\n",
       "      <th>train</th>\n",
       "      <td>6335</td>\n",
       "      <td>6335</td>\n",
       "      <td>6335</td>\n",
       "      <td>6335</td>\n",
       "      <td>3189</td>\n",
       "      <td>6335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>1118</td>\n",
       "      <td>1118</td>\n",
       "      <td>1118</td>\n",
       "      <td>1118</td>\n",
       "      <td>585</td>\n",
       "      <td>1118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">News/Media</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>2091</td>\n",
       "      <td>2091</td>\n",
       "      <td>2091</td>\n",
       "      <td>2091</td>\n",
       "      <td>1104</td>\n",
       "      <td>2091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>369</td>\n",
       "      <td>369</td>\n",
       "      <td>369</td>\n",
       "      <td>369</td>\n",
       "      <td>208</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Paris Agreement</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">9</th>\n",
       "      <th>train</th>\n",
       "      <td>2325</td>\n",
       "      <td>2325</td>\n",
       "      <td>2325</td>\n",
       "      <td>2325</td>\n",
       "      <td>1099</td>\n",
       "      <td>2325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>212</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">PolarBear/Wildlife</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
       "      <th>train</th>\n",
       "      <td>3181</td>\n",
       "      <td>3181</td>\n",
       "      <td>3181</td>\n",
       "      <td>3181</td>\n",
       "      <td>1626</td>\n",
       "      <td>3181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>561</td>\n",
       "      <td>561</td>\n",
       "      <td>561</td>\n",
       "      <td>561</td>\n",
       "      <td>296</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Politics/Policy/Law</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>train</th>\n",
       "      <td>4273</td>\n",
       "      <td>4273</td>\n",
       "      <td>4273</td>\n",
       "      <td>4273</td>\n",
       "      <td>1998</td>\n",
       "      <td>4273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>754</td>\n",
       "      <td>754</td>\n",
       "      <td>754</td>\n",
       "      <td>754</td>\n",
       "      <td>382</td>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">xxForeign</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">7</th>\n",
       "      <th>train</th>\n",
       "      <td>3176</td>\n",
       "      <td>3176</td>\n",
       "      <td>3176</td>\n",
       "      <td>3176</td>\n",
       "      <td>1623</td>\n",
       "      <td>3176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>561</td>\n",
       "      <td>561</td>\n",
       "      <td>561</td>\n",
       "      <td>561</td>\n",
       "      <td>292</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        Document_No  \\\n",
       "Dominant_Topic                         label data_type                \n",
       "Action/Efforts/Awareness               6     train            16549   \n",
       "                                             val               2920   \n",
       "Arctic/Icecap                          3     train             6632   \n",
       "                                             val               1170   \n",
       "Belief/Sentiment                       2     train             8738   \n",
       "                                             val               1542   \n",
       "ClimateChangeIsReal/FightClimateChange 4     train            21571   \n",
       "                                             val               3807   \n",
       "Destruction/Consequences               0     train            10129   \n",
       "                                             val               1788   \n",
       "Energy/Emission/Carbon/                10    train             6335   \n",
       "                                             val               1118   \n",
       "News/Media                             1     train             2091   \n",
       "                                             val                369   \n",
       "Paris Agreement                        9     train             2325   \n",
       "                                             val                410   \n",
       "PolarBear/Wildlife                     8     train             3181   \n",
       "                                             val                561   \n",
       "Politics/Policy/Law                    5     train             4273   \n",
       "                                             val                754   \n",
       "xxForeign                              7     train             3176   \n",
       "                                             val                561   \n",
       "\n",
       "                                                        Topic_Perc_Contrib  \\\n",
       "Dominant_Topic                         label data_type                       \n",
       "Action/Efforts/Awareness               6     train                   16549   \n",
       "                                             val                      2920   \n",
       "Arctic/Icecap                          3     train                    6632   \n",
       "                                             val                      1170   \n",
       "Belief/Sentiment                       2     train                    8738   \n",
       "                                             val                      1542   \n",
       "ClimateChangeIsReal/FightClimateChange 4     train                   21571   \n",
       "                                             val                      3807   \n",
       "Destruction/Consequences               0     train                   10129   \n",
       "                                             val                      1788   \n",
       "Energy/Emission/Carbon/                10    train                    6335   \n",
       "                                             val                      1118   \n",
       "News/Media                             1     train                    2091   \n",
       "                                             val                       369   \n",
       "Paris Agreement                        9     train                    2325   \n",
       "                                             val                       410   \n",
       "PolarBear/Wildlife                     8     train                    3181   \n",
       "                                             val                       561   \n",
       "Politics/Policy/Law                    5     train                    4273   \n",
       "                                             val                       754   \n",
       "xxForeign                              7     train                    3176   \n",
       "                                             val                       561   \n",
       "\n",
       "                                                        Keywords   Text  \\\n",
       "Dominant_Topic                         label data_type                    \n",
       "Action/Efforts/Awareness               6     train         16549  16549   \n",
       "                                             val            2920   2920   \n",
       "Arctic/Icecap                          3     train          6632   6632   \n",
       "                                             val            1170   1170   \n",
       "Belief/Sentiment                       2     train          8738   8738   \n",
       "                                             val            1542   1542   \n",
       "ClimateChangeIsReal/FightClimateChange 4     train         21571  21571   \n",
       "                                             val            3807   3807   \n",
       "Destruction/Consequences               0     train         10129  10129   \n",
       "                                             val            1788   1788   \n",
       "Energy/Emission/Carbon/                10    train          6335   6335   \n",
       "                                             val            1118   1118   \n",
       "News/Media                             1     train          2091   2091   \n",
       "                                             val             369    369   \n",
       "Paris Agreement                        9     train          2325   2325   \n",
       "                                             val             410    410   \n",
       "PolarBear/Wildlife                     8     train          3181   3181   \n",
       "                                             val             561    561   \n",
       "Politics/Policy/Law                    5     train          4273   4273   \n",
       "                                             val             754    754   \n",
       "xxForeign                              7     train          3176   3176   \n",
       "                                             val             561    561   \n",
       "\n",
       "                                                        hashtags  Topic_no  \n",
       "Dominant_Topic                         label data_type                      \n",
       "Action/Efforts/Awareness               6     train          7559     16549  \n",
       "                                             val            1342      2920  \n",
       "Arctic/Icecap                          3     train          2834      6632  \n",
       "                                             val             491      1170  \n",
       "Belief/Sentiment                       2     train          3791      8738  \n",
       "                                             val             635      1542  \n",
       "ClimateChangeIsReal/FightClimateChange 4     train          6166     21571  \n",
       "                                             val            1082      3807  \n",
       "Destruction/Consequences               0     train          2821     10129  \n",
       "                                             val             505      1788  \n",
       "Energy/Emission/Carbon/                10    train          3189      6335  \n",
       "                                             val             585      1118  \n",
       "News/Media                             1     train          1104      2091  \n",
       "                                             val             208       369  \n",
       "Paris Agreement                        9     train          1099      2325  \n",
       "                                             val             212       410  \n",
       "PolarBear/Wildlife                     8     train          1626      3181  \n",
       "                                             val             296       561  \n",
       "Politics/Policy/Law                    5     train          1998      4273  \n",
       "                                             val             382       754  \n",
       "xxForeign                              7     train          1623      3176  \n",
       "                                             val             292       561  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n",
    "                                                  df.label.values, \n",
    "                                                  test_size=0.15, \n",
    "                                                  random_state=42, \n",
    "                                                  stratify=df.label.values)\n",
    "\n",
    "df['data_type'] = ['not_set']*df.shape[0]\n",
    "\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "df.groupby(['Dominant_Topic', 'label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/govin/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "                                          \n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='train'].Text.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val'].Text.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df[df.data_type=='train'].label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df[df.data_type=='val'].label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "                  \n",
    "epochs = 2\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b80d2133c97407da4b969c21ac2cb96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 1'), FloatProgress(value=0.0, max=2834.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 1.787939829548102\n",
      "Validation loss: 1.567190859824419\n",
      "F1 Score (Weighted): 0.4602538236563847\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 2'), FloatProgress(value=0.0, max=2834.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 1.4202015810188013\n",
      "Validation loss: 1.3975628977119923\n",
      "F1 Score (Weighted): 0.5290076500645929\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(device)\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals\n",
    "    \n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
